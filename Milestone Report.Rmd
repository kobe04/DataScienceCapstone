---
title: "Milestone Report"
author: "K. van Splunter"
date: "April 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = "")
```
## Introduction

This report provides an introduction to the Capstone Project for the Data Science Specialization on [Coursera][1]. The overall goal of the project is to develop a word predictor. It will be deployed as a Shiny app and be accompanied by a Presentation that pitches the app. The project is done in partnership with [SwiftKey][2].

The goal of this report is to provide some exploratory analysis of the data.

To keep this report tidy and short, most of the code is not included. However, the code can easily be found and checked [here][3]!

## Loading the initial data

This part of the code downloads the data to the computer and unzips it. It checks if the data is already downloaded and unzipped. If the names of the original files are not changed, this process will only download the files once. Furthermore, the necessary packages are loaded.

```{r downloadData, echo = TRUE}
# Create a directory for the data
if(!file.exists("./Data")){
    dir.create("./Data")
}
# Download the data
URL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if(!file.exists("./Data/SwiftkeyData.zip")){
    download.file(URL, destfile = "./Data/SwiftkeyData.zip")
}
# Unzip the data
if(!file.exists("./Data/final")){
    unzip(zipfile = "./Data/SwiftkeyData.zip", exdir = "./Data")
}
dir("./Data/final/en_US")

# Calculate the size of the files
sizeBlogs <- round(file.size("./Data/final/en_US/en_US.blogs.txt")/(10^6), 3)
sizeNews <- round(file.size("./Data/final/en_US/en_US.news.txt")/(10^6), 3)
sizeTwitter <- round(file.size("./Data/final/en_US/en_US.twitter.txt")/(10^6), 3)

library(readr); suppressPackageStartupMessages(library(quanteda)); library(ggplot2)
```

There are three files. They respectively hold texts about blogs, news, and tweets.
The size of the files is quite big. The data of the blogs is `r sizeBlogs` MB.
The news-data is `r sizeNews` MB. The twitter-data is `r sizeTwitter` MB.
Because the files are this big, the files are sampled and a new (smaller) datafile is created.

## Creating a sample

To assure that this project is reproducible, the seed is set. Then, the samples are taken and combined into one new file. It is decided that about 10% of the data in the files is included in the sample. These are the first three entries in the sample.

```{r Sampling}
# Set the seed
set.seed(824039)

# Read in the original data
con <- file("./Data/final/en_US/en_US.twitter.txt", encoding = "UTF-8", method = "r")
twitterData <- readLines(con, skipNul = TRUE)
close(con)
con <- file("./Data/final/en_US/en_US.blogs.txt", encoding = "UTF-8", method = "r")
blogData <- readLines(con, skipNul = TRUE)
close(con)
con <- file("./Data/final/en_US/en_US.news.txt", encoding = "UTF-8", method = "r")
newsData <- read_lines(con, skip = 0)
rm(con)

# Create the new file
sampleData <- c(sample(blogData, round(length(blogData) * 0.1)),
                sample(newsData, round(length(newsData) * 0.1)),
                sample(twitterData, round(length(twitterData) * 0.1)))
head(sampleData, 3)
writeLines(sampleData, "./Data/sampleData.txt")
```


## Exploratory analysis

First, several characteristics of the original data and the sample are shown. These are the size, the length (how many entries), and the (estimated) total amount of words.

```{r DataCharacteristics}
# Get size of the sample
sizeSample <- round(file.size("./Data/sampleData.txt")/(10^6), 3)

# Get amount of lines of the data
lengthBlog <- length(blogData)
lengthNews <- length(newsData)
lengthTwitter <- length(twitterData)
lengthSample <- length(sampleData)

# Get amount of words in data
blogWords <- sum(sapply(gregexpr("\\w+", blogData), length))
newsWords <- sum(sapply(gregexpr("\\w+", newsData), length))
twitterWords <- sum(sapply(gregexpr("\\w+", twitterData), length))
sampleWords <- sum(sapply(gregexpr("\\w+", sampleData), length))

# Create dataframe with overview of characteristics
filesOverview <- data.frame(DataSource = c("Blogs", "News", "Twitter", "Sample"),
                            Size = c(sizeBlogs, sizeNews, sizeTwitter, sizeSample),
                            Length = c(lengthBlog, lengthNews, lengthTwitter, lengthSample),
                            Words = c(blogWords, newsWords, twitterWords, sampleWords))
filesOverview
```

For further analysis, the sample is cleaned. As the first three entries show, there are numbers, punctuation and capital letters. To create a good prediction model, these features need to be removed. Furthermore, the stopwords and several profanity words are also removed. The profanities that are removed, are the *seven words you can never say on television*, a list [created][4] by George Carlin in 1972.
All the necessary transformations are made with the quanteda-package. You can find more about this package [here][5]. The list of stopwords is part of the quanteda-package.

```{r CleanSample}
# Read in profanity list
rm(list = c("blogData", "newsData", "twitterData"))

profanityList <- c("shit", "piss", "fuck", "cunt", "cocksucker", "motherfucker", "tits")

# Create corpus of sample
sampleData <- char_tolower(sampleData)
sampleCorpus <- corpus(sampleData)
rm(sampleData)
```

To provide some exploratory statistics of the sample data, a Document-Feature Matrix is created. This plot shows the top-50 words in the sample data. It stands out that the words *said* and *just* appear the most in the data. 
```{r DFM}
# Create Document-Feature Matrix
dFM <- dfm(sampleCorpus, remove = c(stopwords("english"), profanityList),
           remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE)

# Create plot to display top-50 words
features_dFM <- textstat_frequency(dFM, n = 50)
features_dFM$feature <- with(features_dFM, reorder(feature, -frequency))
g <- ggplot(features_dFM, aes(x = feature, y = frequency))
g <- g + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1),
                              panel.background = element_rect(fill = "white"),
                              panel.grid = element_line(colour = "grey"))
g
```

### Bigram

This plot shows the top-50 bigrams in the sample. the combinations "*right now*" and "*new york*" appear the most in the text.
```{r bigram}
# Tokenize the sampleCorpus
tokenized <- tokens(sampleCorpus, what = "word", remove_numbers = TRUE, remove_punct = TRUE,
                    remove_separators = TRUE)
tokenized <- tokens_remove(tokenized, stopwords("english"))
tokenized <- tokens_remove(tokenized, profanityList)

# Create bigram and DFM
tokenBi <- tokens_ngrams(tokenized, n = 2)
dFMBi <- dfm(tokenBi)

# Create histogram of most appearing bigrams
features_dFMBi <- textstat_frequency(dFMBi, n = 50)
features_dFMBi$feature <- with(features_dFMBi, reorder(feature, -frequency))
gBi <- ggplot(features_dFMBi, aes(x = feature, y = frequency))
gBi <- gBi + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1),
                              panel.background = element_rect(fill = "white"),
                              panel.grid = element_line(colour = "grey"))
gBi
```

### Trigram

The following plot shows the top-50 trigrams in the sample. The combinations "*let us know*" and "*new york city*" appear the most in the sample.
```{r trigram}
# Create trigram and DFM
tokenTri <- tokens_ngrams(tokenized, n = 3)
dFMTri <- dfm(tokenTri)

# Create histogram of most appearing trigrams
features_dFMTri <- textstat_frequency(dFMTri, n = 50)
features_dFMTri$feature <- with(features_dFMTri, reorder(feature, -frequency))
gTri <- ggplot(features_dFMTri, aes(x = feature, y = frequency))
gTri <- gTri + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1),
                              panel.background = element_rect(fill = "white"),
                              panel.grid = element_line(colour = "grey"))
gTri
```

## Interesting Findings

The plots give an impression of the data. In general, they show that there is some further work to be done with cleaning the data. In the first plot, the 'word' "*rt*" appears about 7,000 times in the sample. However, the question rises to what extent this is actually a word.
The plot of the trigram shows that the combinations "*happy mother's day*" and "*happy mothers day*" are both showing up. However, this should be put in one set.  

Furthermore, right now, the stopwords are removed from the sample, but stopwords are an important part of a language. Therefore, it is worth considering to not exclude these words.

## Next steps

The next step is to create a prediction algorithm. This algorithm should not only be based on the combination of words found in the data, but also take non-appearing combinations into account. This calls for an approach of **smoothing** or **discounting**. One way to do this is by using a [back-off model][6].  
it is important to keep the size and runtime of the model in mind, because it has to be deployed on the [shinyapps][7] server. This means that it cannot be more than 1 Gb of RAM.



[1]: https://www.coursera.org/specializations/jhu-data-science "Coursera"
[2]: https://www.microsoft.com/en-us/swiftkey?rtc=1&activetab=pivot_1%3aprimaryr2 "SwiftKey"
[3]: https://github.com/kobe04/DataScienceCapstone "here"
[4]: https://en.wikipedia.org/wiki/Seven_dirty_words "created"
[5]: https://quanteda.io/index.html "here"
[6]: https://en.wikipedia.org/wiki/Katz%27s_back-off_model "back-off model"
[7]: https://www.shinyapps.io/ "shinyapps"